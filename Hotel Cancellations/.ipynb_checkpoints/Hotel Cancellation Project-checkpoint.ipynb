{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a model to predict hotel cancellation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('hotel_bookings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data.describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "raw_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = raw_data.copy()\n",
    "#data = data.drop(['arrival_date_year'],['arrival_date_month'],['arrival_date_week_number'],['arrival_date_day_of_month'], axis=1)\n",
    "data = data.drop(['arrival_date_year','arrival_date_month','arrival_date_week_number','arrival_date_day_of_month'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we quickly find out about bookings and cancellations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=raw_data['reservation_status'],data=raw_data, hue='hotel', palette = 'hls')\n",
    "plt.xlabel('Reservation Status', size=12)\n",
    "plt.ylabel('Total Bookings', size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     A reasonably large proportion of bookings result in cancellation. Majority of which are linked to City Hotel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a close look at the above statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by: https://robertmitchellv.com/blog-bar-chart-annotations-pandas-mpl.html \n",
    "ax= raw_data['reservation_status'].value_counts().plot(kind='bar', figsize=(5,3),\n",
    "                                                       color=['coral','green','slateblue'], fontsize=9);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title('Booking By Status', fontsize=9)\n",
    "ax.set_ylabel('Total Booking', size=9)\n",
    "\n",
    "# create a list to collect the plt.patches data\n",
    "totals=[]\n",
    "\n",
    "# find the values and append to list\n",
    "for i in ax.patches:\n",
    "    totals.append(i.get_height())\n",
    "    \n",
    "# set individual bar lables using above list\n",
    "total = sum(totals)\n",
    "\n",
    "# set individual bar labels using above list\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_x()-.0000001,i.get_height()+.5, \\\n",
    "            str(round((i.get_height()/total)*100,2))+'%', fontsize=12,\n",
    "            color='dimgrey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "             Cancellation accounts for 36 per cent of the bookings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data.groupby(\"is_canceled\")[\"reservation_status\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancelled_data = raw_data[(raw_data['reservation_status'] == 'Canceled')]\n",
    "#cancelled_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data['cancelled_hotel'] = np.where(raw_data['reservation_status']=='canceled',raw_data['hotel'],np.nan)\n",
    "ax= cancelled_data['hotel'].value_counts().plot(kind='bar', figsize=(5,3),\n",
    "                                                       color=['coral','green','slateblue'], fontsize=9);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title('Cancellation By Hotel', fontsize=9)\n",
    "ax.set_ylabel('Total Cancellation', size=9)\n",
    "\n",
    "# create a list to collect the plt.patches data\n",
    "totals=[]\n",
    "\n",
    "# find the values and append to list\n",
    "for i in ax.patches:\n",
    "    totals.append(i.get_height())\n",
    "    \n",
    "# set individual bar lables using above list\n",
    "total = sum(totals)\n",
    "\n",
    "# set individual bar labels using above list\n",
    "for i in ax.patches:\n",
    "    ax.text(i.get_x()-.0000001,i.get_height()+.5, \\\n",
    "            str(round((i.get_height()/total)*100,2))+'%', fontsize=12,\n",
    "            color='dimgrey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     3 out of 4 cancellations were linked to City Hotel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is cancellation rate high during certain periods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pd.crosstab(raw_data.arrival_date_month,raw_data.is_canceled).apply(lambda r: r/r.sum(), axis=1).plot(kind='line')\n",
    "#.reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cancellation % by month\n",
    "cancelled_perc = (pd.crosstab(cancelled_data.arrival_date_month,cancelled_data.hotel)\n",
    "                /pd.crosstab(raw_data.arrival_date_month,raw_data.hotel)*100)\n",
    "\n",
    "cancelled_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# order by month:\n",
    "ordered_months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "# plot cancellation in % by month\n",
    "ax = cancelled_perc.reindex(ordered_months).plot(kind=\"line\" , legend=True,  figsize=(7, 5)\n",
    "                                                )\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Cancellations %\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "plt.title(\"Cancellations per month\", fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      Cancellation rate is at its peak during holiday season (between June and September) for Resort Hotel.\n",
    "      Whereas cancellation rate for City Hotel peaks in April, then gradually drops over the summer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about trend over the years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cancellation % by year\n",
    "cancelled_perc_yr = (pd.crosstab(cancelled_data.arrival_date_year,cancelled_data.hotel)\n",
    "                    /pd.crosstab(raw_data.arrival_date_year,raw_data.hotel)*100)\n",
    "\n",
    "cancelled_perc_yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cancellation rate by year\n",
    "ax = cancelled_perc_yr.plot(kind=\"bar\" , legend=True,  figsize=(7, 5)\n",
    "                                                )\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Cancellations %\")\n",
    "ax.legend(loc=\"upper center\", fontsize=9)\n",
    "plt.title(\"Cancellations per month\", fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Around 40 per cent of City Hotel bookings result in cancellation, while the rate for Resort Hotel has steadily increased between 2015 and 2017, from the rate of 25 per cent to 30 per cent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing key factors for cancelled bookings?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This sections will look at each feature with the aim to establish relevance and correlation with target vablue, thus determining whether it would be a good predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect null values  and remove irrelevant columns\n",
    "fig,axes = plt.subplots(1,1,figsize=(9,5))\n",
    "sns.heatmap(data.isna(),yticklabels=False,cbar=False,cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inspect null values again, but as a percentage of overall dataset\n",
    "round((data.isnull().sum() / (data.isnull().sum()+data.count()))*100,2).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data.country#.groupby('Region')['country'].unique()\n",
    "#raw_data.country[(raw_data['Country_Name'].isna())].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fact that 94% of company field is null, it would be irrelevant and useless as a feature. agent field has also a high percentage of missing value however it will be kept here as majority of bookings are placed by individuals and not companies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove company field\n",
    "data = data.drop('company', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing numerical data\n",
    "https://www.kaggle.com/rahulvv/cancel-or-not-accuracy-precision-recall-of-1 In [31] - [35]\n",
    "\n",
    "Step 1: List variables and establish relationship using corr(), then remove irrelevant ones.\n",
    "\n",
    "        Merge fields if necessary, e.g. children & babies column/stay in ??weekdays and weekends??\n",
    "        Then re-check relationship using corr(), and thrn remove irrelevant ones\n",
    "        \n",
    "Step 2: Plot data using: distribution like in Udemy course or boxplot like in kaggle example\n",
    "\n",
    "Step 3: Identify outliers, then remove or update outliers values. e.g. in Children field, replace null with 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's have a look at relationships within the dataset.\n",
    "# The approach adopted here would allow a quick identification of important features for the model.\n",
    "fig,axes = plt.subplots(1,1,figsize=(10,7))\n",
    "sns.heatmap(data.corr())\n",
    "#sns.heatmap(data.corr(), xticklabels=corr.columns, yticklabels=corr.columns, cmap=sns.diverging_palette(220, 10, as_cmap=True))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now a look at relationship of other features with target, with focus on numerical features.\n",
    "cancel_corr = data.corr()[\"is_canceled\"]\n",
    "cancel_corr.abs().sort_values(ascending=False)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was expecting daily rate (adr) to be one of (if not) the most important numerical feature. After taking a closer look however, adr isn't even included in the top 10 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's merge 'children' and 'babies' and see if correlation improves:\n",
    "data['young_person'] = data['children'] + data['babies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 5, 'display.max_columns', None): \n",
    "    display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's see if young person has a better correlation with target\n",
    "cancel_corr = data.corr()[\"is_canceled\"]\n",
    "cancel_corr.abs().sort_values(ascending=False)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# young person field doesn't make any improvement, therefore we'll remove this feature from the dataset:\n",
    "data = data.drop('young_person', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 5, 'display.max_columns', None): \n",
    "    display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot numerical data with the aim to understand how variables are distributed...\n",
    "# ...thus revealing anomalies such as outliers.\n",
    "# Then remove outliers if there are only few of them, or change their values to min/max/mean/median if they're alot.\n",
    "#sns.distplot(data['lead_time'])\n",
    "sns.distplot(data.lead_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.lead_time.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove outliers in leadtime by \n",
    "data.lead_time[(data.lead_time > data.lead_time.quantile(0.95))].count() / data.lead_time.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# it appears that there are outliers situated around higher lead time, and it best to address this...\n",
    "# some may decide to remove those outliers but these data will be kept here, by updating any value above 95th percentile\n",
    "data.lead_time = np.where(data.lead_time > data.lead_time.quantile(0.95), data.lead_time.quantile(0.95),data.lead_time)\n",
    "data.lead_time.describe(include='all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "data.groupby(\"required_car_parking_spaces\")[\"required_car_parking_spaces\"].value_counts()/data.total_of_special_requests.count()\n",
    ", data.groupby(\"required_car_parking_spaces\")[\"required_car_parking_spaces\"].value_counts()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the fact that less than 1% require more than 1 parking space, it's worth simplifying this feature\n",
    "# ...by grouping this feature into two options: those with a parking space requirement and those with none\n",
    "data.required_car_parking_spaces = np.where(data.required_car_parking_spaces > 0,1,0)\n",
    "data.groupby(\"required_car_parking_spaces\")[\"required_car_parking_spaces\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's remove fields that we don't need:\n",
    "data = data.drop(['arrival_date_year','arrival_date_week_number','arrival_date_day_of_month','agent'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of remaining/relevant numerical variables\n",
    "[var for var in data.columns if data[var].dtypes!='object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining of numerical fields will remain unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing categorical data\n",
    "\n",
    "Step 1: List variables and remove irrelevant ones.\n",
    "\n",
    "Step 2: Create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing value again, in case we've missed any.\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Region = np.where(data.Region.isna(),'Unspecified',data.Region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.children = np.where(data.children.isna(),0,data.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[var for var in data.columns if data[var].dtypes=='object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['arrival_date_month','distribution_channel','reservation_status','reservation_status_date','Arrival_Date','Country_Name','country','Region_Name'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a list of remaining/relevant categorical variables\n",
    "[var for var in data.columns if data[var].dtypes=='object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 5, 'display.max_columns', None): \n",
    "    display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for categorical data\n",
    "data_inc_dummies = pd.get_dummies(data, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 15, 'display.max_columns', None): \n",
    "    display(data_inc_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of EDA and data cleanse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing cleansed dataset\n",
    "[356] / [458]\n",
    "\n",
    "Step 1: Balance the dataset (between cancelled and not cancelled), to get a split of around 50% \n",
    "\n",
    "Step 2: Split inputs and targets\n",
    "\n",
    "Step 3: Standardise (non dummies) inputs \n",
    "\n",
    "Step 4: Shuffle the data (inputs & outputs)\n",
    "\n",
    "Step 5: Split the dataset into train, validation, and test\n",
    "\n",
    "Step 6: Save the three datasets in *.npz (for neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed = data_inc_dummies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 15, 'display.max_columns', None): \n",
    "    display(data_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on EDA, we're aware dataset isn't balanced. There are less cancelled bookings than non-cancelled bookings.\n",
    "# Let's remind ourselves of the split again\n",
    "data_preprocessed.is_canceled.sum()/data_preprocessed.is_canceled.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed.groupby(\"is_canceled\")[\"is_canceled\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The aim is to have a \"balanced\" dataset, therefore some input/target pairs would have to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split data between cancelled and non-cancelled\n",
    "cancelled_data_all = data_preprocessed[data_preprocessed.is_canceled == 1]\n",
    "confirmed_data_all = data_preprocessed[data_preprocessed.is_canceled == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, determine the difference between the two subsets\n",
    "to_remove = cancelled_data_all.shape[0] - confirmed_data_all.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, remove some data in non-cancelled data, equal to the difference between the two subsets\n",
    "confirmed_data_all = confirmed_data_all[:to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, it's time to merge the subsets into a balanced dataset\n",
    "balanced_dataset = pd.concat([cancelled_data_all, confirmed_data_all]\n",
    "                             , sort=True\n",
    "                             #, ignore_index=True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None): \n",
    "    display(balanced_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's confirm this is in fact the case:\n",
    "balanced_dataset.is_canceled.sum()/balanced_dataset.is_canceled.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Split the dataset between inputs and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a list of all columns, in order to rearrange them\n",
    "balanced_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, rearrange so that target variable is first in the list\n",
    "balanced_dataset.columns = ['is_canceled', 'Region_Americas', 'Region_Asia', 'Region_Europe', 'Region_Oceania',\n",
    "       'Region_Unspecified', 'adr', 'adults', 'assigned_room_type_B',\n",
    "       'assigned_room_type_C', 'assigned_room_type_D', 'assigned_room_type_E',\n",
    "       'assigned_room_type_F', 'assigned_room_type_G', 'assigned_room_type_H',\n",
    "       'assigned_room_type_I', 'assigned_room_type_K', 'assigned_room_type_L',\n",
    "       'assigned_room_type_P', 'babies', 'booking_changes', 'children',\n",
    "       'customer_type_Group', 'customer_type_Transient',\n",
    "       'customer_type_Transient-Party', 'days_in_waiting_list',\n",
    "       'deposit_type_Non Refund', 'deposit_type_Refundable',\n",
    "       'hotel_Resort Hotel', 'is_repeated_guest', 'lead_time',\n",
    "       'market_segment_Complementary', 'market_segment_Corporate',\n",
    "       'market_segment_Direct', 'market_segment_Groups',\n",
    "       'market_segment_Offline TA/TO', 'market_segment_Online TA',\n",
    "       'market_segment_Undefined', 'meal_FB', 'meal_HB', 'meal_SC',\n",
    "       'meal_Undefined', 'previous_bookings_not_canceled',\n",
    "       'previous_cancellations', 'required_car_parking_spaces',\n",
    "       'reserved_room_type_B', 'reserved_room_type_C', 'reserved_room_type_D',\n",
    "       'reserved_room_type_E', 'reserved_room_type_F', 'reserved_room_type_G',\n",
    "       'reserved_room_type_H', 'reserved_room_type_L', 'reserved_room_type_P',\n",
    "       'stays_in_week_nights', 'stays_in_weekend_nights',\n",
    "       'total_of_special_requests']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 5, 'display.max_columns', None): \n",
    "    display(balanced_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split inputs and targets data\n",
    "inputs_unscaled = balanced_dataset.iloc[:,1:]\n",
    "targets         = balanced_dataset.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the inputs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# standardize the inputs\n",
    "\n",
    "# standardization is one of the most common preprocessing tools\n",
    "# since data of different magnitude (scale) can be biased towards high values,\n",
    "# we want all inputs to be of similar magnitude\n",
    "# this is a peculiarity of machine learning in general - most (but not all) algorithms do badly with unscaled data\n",
    "\n",
    "# a very useful module we can use is StandardScaler \n",
    "# it has much more capabilities than the straightforward 'preprocessing' method\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# we will create a variable that will contain the scaling information for this particular dataset\n",
    "# documentation: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "# define scaler as an object\n",
    "df_feature_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import the libraries needed to create the Custom Scaler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the Custom Scaler class\n",
    "\n",
    "class CustomScaler(BaseEstimator,TransformerMixin): \n",
    "    \n",
    "    # init or what information we need to declare a CustomScaler object\n",
    "    # and what is calculated/declared as we do\n",
    "    \n",
    "    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n",
    "        \n",
    "        # scaler is nothing but a Standard Scaler object\n",
    "        self.scaler = StandardScaler(copy,with_mean,with_std)\n",
    "        # with some columns 'twist'\n",
    "        self.columns = columns\n",
    "        self.mean_ = None\n",
    "        self.var_ = None\n",
    "        \n",
    "    \n",
    "    # the fit method, which, again based on StandardScale\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns], y)\n",
    "        self.mean_ = np.mean(X[self.columns])\n",
    "        self.var_ = np.var(X[self.columns])\n",
    "        return self\n",
    "    \n",
    "    # the transform method which does the actual scaling\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        \n",
    "        # record the initial order of the columns\n",
    "        init_col_order = X.columns\n",
    "        \n",
    "        # scale all features that you chose when creating the instance of the class\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n",
    "        \n",
    "        # declare a variable containing all information that was not scaled\n",
    "        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n",
    "        \n",
    "        # return a data frame which contains all scaled features and all 'not scaled' features\n",
    "        # use the original order (that you recorded in the beginning)\n",
    "        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check what are all columns that we've got\n",
    "inputs_unscaled.columns.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# choose the columns to scale/omit\n",
    "    \n",
    "# select the columns to omit (All dummy variables)\n",
    "columns_to_omit = ['Region_Americas', 'Region_Asia', 'Region_Europe',\n",
    "       'Region_Oceania', 'Region_Unspecified',\n",
    "       'assigned_room_type_B', 'assigned_room_type_C',\n",
    "       'assigned_room_type_D', 'assigned_room_type_E',\n",
    "       'assigned_room_type_F', 'assigned_room_type_G',\n",
    "       'assigned_room_type_H', 'assigned_room_type_I',\n",
    "       'assigned_room_type_K', 'assigned_room_type_L',\n",
    "       'assigned_room_type_P',\n",
    "       'customer_type_Group', 'customer_type_Transient',\n",
    "       'customer_type_Transient-Party',\n",
    "       'deposit_type_Non Refund', 'deposit_type_Refundable',\n",
    "       'hotel_Resort Hotel', 'is_repeated_guest',\n",
    "       'market_segment_Complementary', 'market_segment_Corporate',\n",
    "       'market_segment_Direct', 'market_segment_Groups',\n",
    "       'market_segment_Offline TA/TO', 'market_segment_Online TA',\n",
    "       'market_segment_Undefined', 'meal_FB', 'meal_HB', 'meal_SC',\n",
    "       'meal_Undefined', 'required_car_parking_spaces',\n",
    "       'reserved_room_type_B', 'reserved_room_type_C',\n",
    "       'reserved_room_type_D', 'reserved_room_type_E',\n",
    "       'reserved_room_type_F', 'reserved_room_type_G',\n",
    "       'reserved_room_type_H', 'reserved_room_type_L',\n",
    "       'reserved_room_type_P']\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# create the columns to scale, based on the columns to omit\n",
    "# use list comprehension to iterate over the list\n",
    "columns_to_scale = [x for x in inputs_unscaled.columns.values if x not in columns_to_omit]\n",
    "columns_to_scale"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputs_to_omit = inputs_unscaled[columns_to_omit]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputs_to_omit"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputs_to_scale = inputs_unscaled[columns_to_scale]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "inputs_to_scale"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# declare a scaler object, specifying the columns to scale\n",
    "df_feature_scaler = CustomScaler(columns_to_scale)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# fit the data (calculate mean and standard deviation); they are automatically stored inside the object \n",
    "df_feature_scaler.fit(inputs_unscaled)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# standardising the data, using the transform method \n",
    "# in the last line, we fitted the data - in other words\n",
    "# we found the internal parameters of a model that will be used to transform data. \n",
    "# transforming applies these parameters to our data\n",
    "# note that when you get new data, you can just call 'scaler' again and transform it in the same way as now\n",
    "scaled_inputs = df_feature_scaler.transform(inputs_unscaled)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# the scaled_inputs are now an ndarray, because sklearn works with ndarrays\n",
    "with pd.option_context('display.max_rows', 15, 'display.max_columns', None): \n",
    "    display(scaled_inputs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check the shape of the inputs\n",
    "scaled_inputs.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# There seems to be additional data for inputs than before, the following will remove these.\n",
    "index_to_remove = [x for x in scaled_inputs.index if x not in targets]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Drop additional data so that inputs and targets are equal\n",
    "inputs = scaled_inputs.drop(scaled_inputs.index[index_to_remove])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####### Method 2 ########\n",
    "\n",
    "Scale all features including dummies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = scaler.fit_transform(inputs_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train & test and shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split so we can split our data into train and test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check how this method works\n",
    "train_test_split(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare 4 variables for the split\n",
    "# Setting an integer to random_state here to make the shuffle pseudo random, so that observations will always be shuffled in the same random way.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(inputs, targets, #train_size = 0.8, \n",
    "                                                                            test_size = 0.2, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the train inputs and targets\n",
    "print (x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the test inputs and targets\n",
    "print (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of EDA and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Bi-Predict Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the LogReg model from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import the 'metrics' module, which includes important metrics we may want to use\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logistic regression object\n",
    "reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the train inputs\n",
    "reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess the train accuracy of the model\n",
    "reg.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the intercept and coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the intercept (bias) of our model\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the coefficients (weights) of our model\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what were the names of our columns\n",
    "inputs_unscaled.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the names of the columns in an ad-hoc variable\n",
    "feature_name = inputs_unscaled.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the coefficients from this table (they will be exported later and will be used in Tableau)\n",
    "# transpose the model coefficients (model.coef_) and throws them into a df (a vertical organization, so that they can be\n",
    "# multiplied by certain matrices later) \n",
    "summary_table = pd.DataFrame (columns=['Feature name'], data = feature_name)\n",
    "\n",
    "# add the coefficient values to the summary table\n",
    "summary_table['Coefficient'] = np.transpose(reg.coef_)\n",
    "\n",
    "# display the summary table\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do a little Python trick to move the intercept to the top of the summary table\n",
    "# move all indices by 1\n",
    "summary_table.index = summary_table.index + 1\n",
    "\n",
    "# add the intercept at index 0\n",
    "summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n",
    "\n",
    "# sort the df by index\n",
    "summary_table = summary_table.sort_index()\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Series called: 'Odds ratio' which will show the.. odds ratio of each feature\n",
    "summary_table['Odds_ratio'] = np.exp(summary_table.Coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the df\n",
    "summary_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the table according to odds ratio\n",
    "# note that by default, the sort_values method sorts values by 'ascending'\n",
    "summary_table.sort_values('Odds_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess the test accuracy of the model\n",
    "reg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the predicted probabilities of each class\n",
    "# the first column shows the probability of a particular observation to be 0, while the second one - to be 1\n",
    "predicted_proba = reg.predict_proba(x_test)\n",
    "\n",
    "# let's check that out\n",
    "predicted_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select ONLY the probabilities referring to 1s\n",
    "predicted_proba[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0]",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
